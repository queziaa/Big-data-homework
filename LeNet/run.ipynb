{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 找不到指定的程序。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in user code:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\urllib3\\connectionpool.py\", line 793, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\urllib3\\connectionpool.py\", line 496, in _make_request\n",
      "    conn.request(\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\urllib3\\connection.py\", line 400, in request\n",
      "    self.endheaders()\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\http\\client.py\", line 1278, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\http\\client.py\", line 1038, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\http\\client.py\", line 976, in send\n",
      "    self.connect()\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\urllib3\\connection.py\", line 238, in connect\n",
      "    self.sock = self._new_conn()\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\urllib3\\connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000029C080AE890>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\requests\\adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\urllib3\\connectionpool.py\", line 847, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\urllib3\\util\\retry.py\", line 515, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000029C080AE890>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\visdom\\__init__.py\", line 756, in _send\n",
      "    return self._handle_post(\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\visdom\\__init__.py\", line 720, in _handle_post\n",
      "    r = self.session.post(url, data=data)\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\requests\\sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"c:\\Users\\quezi\\.conda\\envs\\weibodatacleaning\\lib\\site-packages\\requests\\adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000029C080AE890>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "Visdom python client failed to establish socket to get messages from the server. This feature is optional and can be disabled by initializing Visdom with `use_incoming_socket=False`, which will prevent waiting for this request to timeout.\n"
     ]
    }
   ],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "import torch.nn as nn  # 导入PyTorch神经网络模块\n",
    "import torch.optim as optim  # 导入PyTorch优化器模块\n",
    "from torchvision.datasets.mnist import MNIST  # 导入PyTorch的MNIST数据集\n",
    "import torchvision.transforms as transforms  # 导入PyTorch的图像预处理模块\n",
    "from torch.utils.data import DataLoader  # 导入PyTorch的数据加载器\n",
    "import visdom  # 导入Visdom库，用于可视化\n",
    "# import onnx  # 导入ONNX库，用于模型的序列化和跨平台部署\n",
    "\n",
    "viz = visdom.Visdom()  # 创建一个Visdom实例\n",
    "\n",
    "# 加载并预处理训练数据\n",
    "data_train = MNIST('./data/mnist',  # 数据集的路径\n",
    "                   download=True,  # 如果数据集不存在，就下载数据集\n",
    "                   transform=transforms.Compose([  # 定义数据预处理操作\n",
    "                       transforms.Resize((32, 32)),  # 将图像大小调整为32x32\n",
    "                       transforms.ToTensor()]))  # 将图像转换为PyTorch张量\n",
    "\n",
    "# 加载并预处理测试数据\n",
    "data_test = MNIST('./data/mnist',  # 数据集的路径\n",
    "                  train=False,  # 加载测试数据\n",
    "                  download=True,  # 如果数据集不存在，就下载数据集\n",
    "                  transform=transforms.Compose([  # 定义数据预处理操作\n",
    "                      transforms.Resize((32, 32)),  # 将图像大小调整为32x32\n",
    "                      transforms.ToTensor()]))  # 将图像转换为PyTorch张量\n",
    "\n",
    "# 创建数据加载器，用于在训练和测试过程中加载数据\n",
    "data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True, num_workers=8)  # 训练数据加载器\n",
    "data_test_loader = DataLoader(data_test, batch_size=1024, num_workers=8)  # 测试数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        self.C1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.S2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.C3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.S4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.C5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
    "        self.F6 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.OUTPUT = nn.Linear(in_features=84, out_features=10)\n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print('input:', x.size())\n",
    "        x = self.C1(x)\n",
    "        # print('C1:', x.size())\n",
    "        x = self.S2(torch.relu(x))\n",
    "        # print('S2:', x.size())\n",
    "        x = self.C3(x)\n",
    "        # print('C3:', x.size())\n",
    "        x = self.S4(torch.relu(x))\n",
    "        # print('S4:', x.size())\n",
    "        x = torch.relu(self.C5(x))\n",
    "        # print('C5:', x.size())\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print('view:', x.size())\n",
    "        x = torch.relu(self.F6(x))\n",
    "        # print('F6:', x.size())\n",
    "        x = self.OUTPUT(x)\n",
    "        # print('OUTPUT:', x.size())\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet5()  # 创建LeNet5模型实例\n",
    "criterion = nn.CrossEntropyLoss()  # 定义损失函数为交叉熵损失\n",
    "optimizer = optim.Adam(net.parameters(), lr=2e-3)  # 定义优化器为Adam，学习率为0.002\n",
    "\n",
    "\n",
    "def train():\n",
    "    net.train()  # 将模型设置为训练模式\n",
    "    for i, (images, labels) in enumerate(data_train_loader):  # 遍历训练数据\n",
    "        optimizer.zero_grad()  # 清零梯度\n",
    "        output = net(images)  # 前向传播\n",
    "        loss = criterion(output, labels)  # 计算损失\n",
    "        # 每10个批次打印一次损失\n",
    "        if i % 10 == 0:\n",
    "            print('Train - , Batch: %d, Loss: %f' % ( i, loss.detach().cpu().item()))\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新权重\n",
    "\n",
    "def test():\n",
    "    net.eval()  # 将模型设置为评估模式\n",
    "    total_correct = 0\n",
    "    avg_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_test_loader):  # 遍历测试数据\n",
    "        output = net(images)  # 前向传播\n",
    "        avg_loss += criterion(output, labels).sum()  # 累计损失\n",
    "        pred = output.detach().max(1)[1]  # 获取预测结果\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()  # 计算正确预测的数量\n",
    "\n",
    "    avg_loss /= len(data_test)  # 计算平均损失\n",
    "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))  # 打印平均损失和准确率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - , Batch: 0, Loss: 0.162940\n",
      "Train - , Batch: 10, Loss: 0.081954\n",
      "Train - , Batch: 20, Loss: 0.111427\n",
      "Train - , Batch: 30, Loss: 0.124486\n",
      "Train - , Batch: 40, Loss: 0.168147\n",
      "Train - , Batch: 50, Loss: 0.067552\n",
      "Train - , Batch: 60, Loss: 0.188095\n",
      "Train - , Batch: 70, Loss: 0.091400\n",
      "Train - , Batch: 80, Loss: 0.051999\n",
      "Train - , Batch: 90, Loss: 0.098939\n",
      "Train - , Batch: 100, Loss: 0.123498\n",
      "Train - , Batch: 110, Loss: 0.103072\n",
      "Train - , Batch: 120, Loss: 0.124200\n",
      "Train - , Batch: 130, Loss: 0.074022\n",
      "Train - , Batch: 140, Loss: 0.148152\n",
      "Train - , Batch: 150, Loss: 0.121036\n",
      "Train - , Batch: 160, Loss: 0.125830\n",
      "Train - , Batch: 170, Loss: 0.095944\n",
      "Train - , Batch: 180, Loss: 0.119061\n",
      "Train - , Batch: 190, Loss: 0.120975\n",
      "Train - , Batch: 200, Loss: 0.050721\n",
      "Train - , Batch: 210, Loss: 0.074454\n",
      "Train - , Batch: 220, Loss: 0.112703\n",
      "Train - , Batch: 230, Loss: 0.047090\n",
      "Train - , Batch: 0, Loss: 0.050594\n",
      "Train - , Batch: 10, Loss: 0.078299\n",
      "Train - , Batch: 20, Loss: 0.060894\n",
      "Train - , Batch: 30, Loss: 0.087493\n",
      "Train - , Batch: 40, Loss: 0.052080\n",
      "Train - , Batch: 50, Loss: 0.063943\n",
      "Train - , Batch: 60, Loss: 0.099776\n",
      "Train - , Batch: 70, Loss: 0.051129\n",
      "Train - , Batch: 80, Loss: 0.065437\n",
      "Train - , Batch: 90, Loss: 0.024329\n",
      "Train - , Batch: 100, Loss: 0.074176\n",
      "Train - , Batch: 110, Loss: 0.088382\n",
      "Train - , Batch: 120, Loss: 0.134840\n",
      "Train - , Batch: 130, Loss: 0.071858\n",
      "Train - , Batch: 140, Loss: 0.109172\n",
      "Train - , Batch: 150, Loss: 0.137873\n",
      "Train - , Batch: 160, Loss: 0.094164\n",
      "Train - , Batch: 170, Loss: 0.059242\n",
      "Train - , Batch: 180, Loss: 0.060252\n",
      "Train - , Batch: 190, Loss: 0.067299\n",
      "Train - , Batch: 200, Loss: 0.075892\n",
      "Train - , Batch: 210, Loss: 0.054151\n",
      "Train - , Batch: 220, Loss: 0.077072\n",
      "Train - , Batch: 230, Loss: 0.065816\n",
      "Test Avg. Loss: 0.000058, Accuracy: 0.981500\n"
     ]
    }
   ],
   "source": [
    "train()  # 训练模型\n",
    "train()  # 训练模型\n",
    "test()  # 测试模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
