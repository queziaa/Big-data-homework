{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "import torch.nn as nn  # 导入PyTorch神经网络模块\n",
    "import torch.optim as optim  # 导入PyTorch优化器模块\n",
    "from torchvision.datasets.mnist import MNIST  # 导入PyTorch的MNIST数据集\n",
    "import torchvision.transforms as transforms  # 导入PyTorch的图像预处理模块\n",
    "from torch.utils.data import DataLoader  # 导入PyTorch的数据加载器\n",
    "import visdom  # 导入Visdom库，用于可视化\n",
    "# import onnx  # 导入ONNX库，用于模型的序列化和跨平台部署\n",
    "\n",
    "viz = visdom.Visdom()  # 创建一个Visdom实例\n",
    "\n",
    "# 加载并预处理训练数据\n",
    "data_train = MNIST('./data/mnist',  # 数据集的路径\n",
    "                   download=True,  # 如果数据集不存在，就下载数据集\n",
    "                   transform=transforms.Compose([  # 定义数据预处理操作\n",
    "                       transforms.Resize((32, 32)),  # 将图像大小调整为32x32\n",
    "                       transforms.ToTensor()]))  # 将图像转换为PyTorch张量\n",
    "\n",
    "# 加载并预处理测试数据\n",
    "data_test = MNIST('./data/mnist',  # 数据集的路径\n",
    "                  train=False,  # 加载测试数据\n",
    "                  download=True,  # 如果数据集不存在，就下载数据集\n",
    "                  transform=transforms.Compose([  # 定义数据预处理操作\n",
    "                      transforms.Resize((32, 32)),  # 将图像大小调整为32x32\n",
    "                      transforms.ToTensor()]))  # 将图像转换为PyTorch张量\n",
    "\n",
    "# 创建数据加载器，用于在训练和测试过程中加载数据\n",
    "data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True, num_workers=8)  # 训练数据加载器\n",
    "data_test_loader = DataLoader(data_test, batch_size=1024, num_workers=8)  # 测试数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.C1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.S2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.C3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.S4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.C5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
    "        self.F6 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.OUTPUT = nn.Linear(in_features=84, out_features=10)\n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print('input:', x.size())\n",
    "        x = self.C1(x)\n",
    "        # print('C1:', x.size())\n",
    "        x = self.S2(torch.relu(x))\n",
    "        # print('S2:', x.size())\n",
    "        x = self.C3(x)\n",
    "        # print('C3:', x.size())\n",
    "        x = self.S4(torch.relu(x))\n",
    "        # print('S4:', x.size())\n",
    "        x = torch.relu(self.C5(x))\n",
    "        # print('C5:', x.size())\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print('view:', x.size())\n",
    "        x = torch.relu(self.F6(x))\n",
    "        # print('F6:', x.size())\n",
    "        x = self.OUTPUT(x)\n",
    "        # print('OUTPUT:', x.size())\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet()  # 创建LeNet模型实例\n",
    "criterion = nn.CrossEntropyLoss()  # 定义损失函数为交叉熵损失\n",
    "optimizer = optim.Adam(net.parameters(), lr=2e-3)  # 定义优化器为Adam，学习率为0.002\n",
    "\n",
    "\n",
    "def train():\n",
    "    net.train()  # 将模型设置为训练模式\n",
    "    for i, (images, labels) in enumerate(data_train_loader):  # 遍历训练数据\n",
    "        optimizer.zero_grad()  # 清零梯度\n",
    "        output = net(images)  # 前向传播\n",
    "        loss = criterion(output, labels)  # 计算损失\n",
    "        # 每10个批次打印一次损失\n",
    "        if i % 10 == 0:\n",
    "            print('Train - , Batch: %d, Loss: %f' % ( i, loss.detach().cpu().item()))\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新权重\n",
    "\n",
    "def test():\n",
    "    net.eval()  # 将模型设置为评估模式\n",
    "    total_correct = 0\n",
    "    avg_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_test_loader):  # 遍历测试数据\n",
    "        output = net(images)  # 前向传播\n",
    "        avg_loss += criterion(output, labels).sum()  # 累计损失\n",
    "        pred = output.detach().max(1)[1]  # 获取预测结果\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()  # 计算正确预测的数量\n",
    "\n",
    "    avg_loss /= len(data_test)  # 计算平均损失\n",
    "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))  # 打印平均损失和准确率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()\n",
    "train()\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
