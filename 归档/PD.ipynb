{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 哈工大社会计算与信息检索研究中心同义词词林扩展版.txt\n",
    "# 读取文件 只保留B开头的\n",
    "# 生成一个字典\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "out = {}\n",
    "with open('哈工大社会计算与信息检索研究中心同义词词林扩展版.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('B'):\n",
    "            line = line.strip()\n",
    "            line = re.sub(r'\\s+', ' ', line)\n",
    "            line = line.split(' ')\n",
    "            out[line[0]] = line[1:]\n",
    "    \n",
    "# 将KEY的前两位作为新的KEY\n",
    "out2 = {}\n",
    "for key in out:\n",
    "    new_key = key[:2]\n",
    "    if new_key not in out2:\n",
    "        out2[new_key] = []\n",
    "    for i in out[key]:\n",
    "        out2[new_key].append(i)\n",
    "        # 将 out2是字典 KEY是类型名字 转化为 X Y变量，X是全部元素，Y是X中元素对于的类型\n",
    "X = []\n",
    "Y_label = []\n",
    "for key in out2:\n",
    "    for i in out2[key]:\n",
    "        X.append(i)\n",
    "        Y_label.append(key)\n",
    "# Y_label是标签类型，Y是同长度的数组，将标签转化为数字 \n",
    "Y = []\n",
    "temp = {}\n",
    "indexA = 0\n",
    "for i in Y_label:\n",
    "    if i not in temp:\n",
    "        temp[i] = indexA\n",
    "        indexA += 1\n",
    "    Y.append(temp[i])\n",
    "    \n",
    "# 保存X和Y\n",
    "with open('X.txt', 'w') as f:\n",
    "    f.write(json.dumps(X))\n",
    "with open('Y.txt', 'w') as f:\n",
    "    f.write(json.dumps(Y))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18849"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\n",
    "model = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# 将X分为6份\n",
    "X = []\n",
    "with open('X.txt', 'r') as f:\n",
    "    X = json.loads(f.read())\n",
    "X = np.array(X)\n",
    "X = np.array_split(X, 6)\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    temp = X[i]\n",
    "    temp = temp.tolist()\n",
    "    input_texts = temp\n",
    "    batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "    outputs = model(**batch_dict)\n",
    "\n",
    "\n",
    "    def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)  #torch.Size([40, 1024])\n",
    "    text = 'embeddings' + str(i) + '.npy'\n",
    "    np.save(text, embeddings.cpu().detach().numpy())\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取    text = 'embeddings' + str(i) + '.npy' \n",
    "out3 = []\n",
    "for i in range(6):\n",
    "    text = 'embeddings' + str(i) + '.npy'\n",
    "    temp = np.load(text)\n",
    "    out3.append(temp)\n",
    "    \n",
    "# type(out3[0]) numpy.ndarray\n",
    "# 合并\n",
    "out4 = np.concatenate(out3, axis=0)\n",
    "# 保存\n",
    "np.save('embeddings.npy', out4)\n",
    "# 读取\n",
    "out5 = np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1437\n",
      "Epoch 0, Loss: 3.3305\n",
      "Accuracy: 0.1437\n",
      "Epoch 10, Loss: 2.4554\n",
      "Accuracy: 0.1437\n",
      "Epoch 20, Loss: 4.5416\n",
      "Accuracy: 0.1437\n",
      "Epoch 30, Loss: 2.7787\n",
      "Accuracy: 0.1437\n",
      "Epoch 40, Loss: 0.5890\n",
      "Accuracy: 0.1437\n",
      "Epoch 50, Loss: 1.8844\n",
      "Accuracy: 0.1437\n",
      "Epoch 60, Loss: 0.3902\n",
      "Accuracy: 0.1437\n",
      "Epoch 70, Loss: 1.9036\n",
      "Accuracy: 0.1437\n",
      "Epoch 80, Loss: 0.0471\n",
      "Accuracy: 0.1437\n",
      "Epoch 90, Loss: 0.1776\n",
      "Accuracy: 0.1437\n",
      "Epoch 100, Loss: 0.0194\n",
      "Accuracy: 0.1437\n",
      "Epoch 110, Loss: 0.0137\n",
      "Accuracy: 0.1437\n",
      "Epoch 120, Loss: 0.0673\n",
      "Accuracy: 0.1437\n",
      "Epoch 130, Loss: 0.1716\n",
      "Accuracy: 0.1437\n",
      "Epoch 140, Loss: 0.0669\n",
      "Accuracy: 0.1437\n",
      "Epoch 150, Loss: 0.0557\n",
      "Accuracy: 0.1437\n",
      "Epoch 160, Loss: 0.0481\n",
      "Accuracy: 0.1437\n",
      "Epoch 170, Loss: 0.1534\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m    119\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m--> 121\u001b[0m trained_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[116], line 97\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(x, y, hidden_dim, output_dim, lr, epochs, batch_size)\u001b[0m\n\u001b[0;32m     94\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y[start:end]\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# 前向和反向传播\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m loss, dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_backward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# 更新权重\u001b[39;00m\n\u001b[0;32m    100\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m update_weights(W1, b1, W2, b2, dW1, db1, dW2, db2, lr)\n",
      "Cell \u001b[1;32mIn[116], line 65\u001b[0m, in \u001b[0;36mforward_backward_pass\u001b[1;34m(x, y, W1, b1, W2, b2)\u001b[0m\n\u001b[0;32m     63\u001b[0m dz2[\u001b[38;5;28mrange\u001b[39m(m), y] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     64\u001b[0m dz2 \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m m\n\u001b[1;32m---> 65\u001b[0m dW2 \u001b[38;5;241m=\u001b[39m \u001b[43ma1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdz2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m db2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dz2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m dz1 \u001b[38;5;241m=\u001b[39m dz2\u001b[38;5;241m.\u001b[39mdot(W2\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m sigmoid_derivative(z1)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义Sigmoid激活函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 定义Sigmoid激活函数的导数\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)\n",
    "\n",
    "# 定义softmax函数，用于输出层的激活\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "# 定义交叉熵损失函数\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(m), y_true])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "# 初始化权重和偏置\n",
    "def init_weights(input_dim, hidden_dim, output_dim):\n",
    "    W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "    b1 = np.zeros((1, hidden_dim))\n",
    "    W2 = np.random.randn(hidden_dim, output_dim) * 0.01\n",
    "    b2 = np.zeros((1, output_dim))\n",
    "    return W1, b1, W2, b2\n",
    "# 更新权重和偏置\n",
    "def update_weights(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "# 定义前向和反向传播过程\n",
    "def forward_backward_pass(x, y, W1, b1, W2, b2):\n",
    "    # 前向传播\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    y_pred = softmax(z2)\n",
    "    # 计算损失\n",
    "    loss = cross_entropy_loss(y_pred, y)\n",
    "    \n",
    "    # 反向传播\n",
    "    m = y.shape[0]\n",
    "    dz2 = y_pred\n",
    "    dz2[range(m), y] -= 1\n",
    "    dz2 /= m\n",
    "    dW2 = a1.T.dot(dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "    dz1 = dz2.dot(W2.T) * sigmoid_derivative(z1)\n",
    "    dW1 = x.T.dot(dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "    \n",
    "    return loss, dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "\n",
    "def train(x, y, hidden_dim, output_dim, lr, epochs, batch_size):\n",
    "    input_dim = x.shape[1]\n",
    "    W1, b1, W2, b2 = init_weights(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    # 计算批次数量\n",
    "    num_batches = x.shape[0] // batch_size\n",
    "    if x.shape[0] % batch_size != 0:\n",
    "        num_batches += 1\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # 打乱顺序\n",
    "        indices = np.random.permutation(x.shape[0])\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "        for j in range(num_batches):\n",
    "            # 获取当前批次的数据\n",
    "            start = j * batch_size\n",
    "            end = min((j + 1) * batch_size, x.shape[0])\n",
    "            x_batch = x[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            \n",
    "            # 前向和反向传播\n",
    "            loss, dW1, db1, dW2, db2 = forward_backward_pass(x_batch, y_batch, W1, b1, W2, b2)\n",
    "            \n",
    "            # 更新权重\n",
    "            W1, b1, W2, b2 = update_weights(W1, b1, W2, b2, dW1, db1, dW2, db2, lr)\n",
    "        if i % 10 == 0:\n",
    "            print(f'Accuracy: {accuracy(x, y, *trained_weights):.4f}')\n",
    "            print(f'Epoch {i}, Loss: {loss:.4f}')\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 示例：使用神经网络进行分类\n",
    "\n",
    "x = out5 = np.load('000000000.npy')\n",
    "with open('Y.txt', 'r') as f:\n",
    "    y = json.loads(f.read())\n",
    "y = np.array(y)\n",
    "\n",
    "hidden_dim = 32\n",
    "output_dim = 18\n",
    "lr = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "trained_weights = train(x, y, hidden_dim, output_dim, lr, epochs, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.4136 Epoch 960, Loss: 7.4448\n",
    "Accuracy: 0.4142 Epoch 961, Loss: 7.4386\n",
    "Accuracy: 0.4148 Epoch 962, Loss: 7.4324\n",
    "Accuracy: 0.4154 Epoch 963, Loss: 7.4263\n",
    "Accuracy: 0.4160 Epoch 964, Loss: 7.4201\n",
    "Accuracy: 0.4166 Epoch 965, Loss: 7.4139\n",
    "Accuracy: 0.4172 Epoch 966, Loss: 7.4078\n",
    "Accuracy: 0.4178 Epoch 967, Loss: 7.4016\n",
    "Accuracy: 0.4184 Epoch 968, Loss: 7.3955\n",
    "Accuracy: 0.4190 Epoch 969, Loss: 7.3893\n",
    "Accuracy: 0.4196 Epoch 970, Loss: 7.3832\n",
    "Accuracy: 0.4202 Epoch 971, Loss: 7.3770\n",
    "Accuracy: 0.4208 Epoch 972, Loss: 7.3709\n",
    "Accuracy: 0.4214 Epoch 973, Loss: 7.3647\n",
    "Accuracy: 0.4220 Epoch 974, Loss: 7.3586\n",
    "Accuracy: 0.4226 Epoch 975, Loss: 7.3525\n",
    "Accuracy: 0.4232 Epoch 976, Loss: 7.3463\n",
    "Accuracy: 0.4238 Epoch 977, Loss: 7.3402\n",
    "Accuracy: 0.4244 Epoch 978, Loss: 7.3341\n",
    "Accuracy: 0.4250 Epoch 979, Loss: 7.3280\n",
    "Accuracy: 0.4256 Epoch 980, Loss: 7.3219\n",
    "Accuracy: 0.4262 Epoch 981, Loss: 7.3158\n",
    "Accuracy: 0.4268 Epoch 982, Loss: 7.3097\n",
    "Accuracy: 0.4274 Epoch 983, Loss: 7.3036\n",
    "Accuracy: 0.4280 Epoch 984, Loss: 7.2975\n",
    "Accuracy: 0.4286 Epoch 985, Loss: 7.2914\n",
    "Accuracy: 0.4292 Epoch 986, Loss: 7.2853\n",
    "Accuracy: 0.4298 Epoch 987, Loss: 7.2792\n",
    "Accuracy: 0.4304 Epoch 988, Loss: 7.2731\n",
    "Accuracy: 0.4310 Epoch 989, Loss: 7.2671\n",
    "Accuracy: 0.4316 Epoch 990, Loss: 7.2610\n",
    "Accuracy: 0.4322 Epoch 991, Loss: 7.2549\n",
    "Accuracy: 0.4328 Epoch 992, Loss: 7.2489\n",
    "Accuracy: 0.4334 Epoch 993, Loss: 7.2428\n",
    "Accuracy: 0.4340 Epoch 994, Loss: 7.2368\n",
    "Accuracy: 0.4346 Epoch 995, Loss: 7.2307\n",
    "Accuracy: 0.4352 Epoch 996, Loss: 7.2247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1437\n"
     ]
    }
   ],
   "source": [
    "# 计算准确率\n",
    "def accuracy(x, y, W1, b1, W2, b2):\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    y_pred = np.argmax(softmax(z2), axis=1)\n",
    "    return np.mean(y_pred == y)\n",
    "pr\n",
    "print(f'Accuracy: {accuracy(x, y, *trained_weights):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibodatacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
